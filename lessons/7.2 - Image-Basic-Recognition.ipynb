{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image-Basic-Recognition_static"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O Sistema deve identificar as provas a serem analisadas, independente de sua posição"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [1]Desta forma haverá um delimitador na câmera, onde analizará apenas as informações contidas em seu espaço"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [2]Desta forma após identificar o padrão, irá contabilizar 3 segundos para salvar uma cópia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alinhando a prova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homografia Estimada : \n",
      " [[-1.00020172e+00  1.15150980e-04  4.84021631e+02]\n",
      " [ 4.97546691e-05 -1.00027973e+00  3.18083266e+02]\n",
      " [ 6.60252792e-07  8.52319484e-07  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    " \n",
    "#função responsável por alinhar as imagens baseado no template pré-configurado\n",
    "def align_images(im1, im2):\n",
    "    MAX_FEATURES = 500\n",
    "    GOOD_MATCH_PERCENT = 0.15\n",
    "    \n",
    "    #Alterando imagens para escola cinza\n",
    "    im1Gray = cv2.cvtColor(im1, cv2.COLOR_BGR2GRAY)\n",
    "    im2Gray = cv2.cvtColor(im2, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    #Detectando características do ORB e calculando descritores.\n",
    "    orb = cv2.ORB_create(MAX_FEATURES)\n",
    "    keypoints1, descriptors1 = orb.detectAndCompute(im1Gray, None)\n",
    "    keypoints2, descriptors2 = orb.detectAndCompute(im2Gray, None)\n",
    "    \n",
    "    # Ocorrências de características\n",
    "    matcher = cv2.DescriptorMatcher_create(cv2.DESCRIPTOR_MATCHER_BRUTEFORCE_HAMMING)\n",
    "    matches = matcher.match(descriptors1, descriptors2, None)\n",
    "\n",
    "    # Ordenando melhores ocorrências\n",
    "    matches.sort(key=lambda x: x.distance, reverse=False)\n",
    "    \n",
    "    # Removendo ocorrências que não possuem melhores porcentagem de reconhecimento\n",
    "    numGoodMatches = int(len(matches) * GOOD_MATCH_PERCENT)\n",
    "    matches = matches[:numGoodMatches]\n",
    "    \n",
    "    # Draw melhores matches\n",
    "    imMatches = cv2.drawMatches(im1, keypoints1, im2, keypoints2, matches, None)\n",
    "    cv2.imshow('imMatches', imMatches)\n",
    "    cv2.waitKey(0)\n",
    "    \n",
    "    \n",
    "    # Extraindo localização de melhores correspondências\n",
    "    points1 = np.zeros((len(matches), 2), dtype=np.float32)\n",
    "    points2 = np.zeros((len(matches), 2), dtype=np.float32)\n",
    "\n",
    "    for i, match in enumerate(matches):\n",
    "        points1[i, :] = keypoints1[match.queryIdx].pt\n",
    "        points2[i, :] = keypoints2[match.trainIdx].pt\n",
    "\n",
    "    # buscando homografia\n",
    "    h, mask = cv2.findHomography(points1, points2, cv2.RANSAC)\n",
    "\n",
    "    # Usando homografia\n",
    "    height, width, channels = im2.shape\n",
    "    im1Reg = cv2.warpPerspective(im1, h, (width, height))\n",
    "\n",
    "    return im1Reg, h\n",
    "\n",
    "# Read reference image\n",
    "refFilename = \"dataset/imgs/gabarito_template_geral2.png\"\n",
    "imReference = cv2.imread(refFilename, cv2.IMREAD_COLOR)\n",
    "\n",
    "# Lendo Imagem a alinhar\n",
    "imFilename = \"dataset/imgs/gabarito_template_geral_preenchido_flip_180.png\"\n",
    "im = cv2.imread(imFilename, cv2.IMREAD_COLOR)\n",
    "\n",
    "# A imagem alinhada será restaurada na imReg. \n",
    "# A homografia estimada será armazenada em h.\n",
    "imReg, h = align_images(im, imReference)\n",
    "\n",
    "cv2.imshow('aligned', imReg)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "print(\"Homografia Estimada : \\n\",  h)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identificando respostas do gabarito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(3.4.1) /io/opencv/modules/features2d/src/draw.cpp:115: error: (-215) !outImage.empty() in function drawKeypoints\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-36f47848fb2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mcropped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtop_left\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbottom_right\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtop_left\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbottom_right\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0manswer_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_question\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcropped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-36f47848fb2a>\u001b[0m in \u001b[0;36mevaluate_question\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m#global template\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mcoordinates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoordinates_question\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcoordinates\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-36f47848fb2a>\u001b[0m in \u001b[0;36mcoordinates_question\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mkeypoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# Criando nova imagem com os elementos identificandos pelo detector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mim_with_keypoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrawKeypoints\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeypoints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"teste\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mim_with_keypoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(3.4.1) /io/opencv/modules/features2d/src/draw.cpp:115: error: (-215) !outImage.empty() in function drawKeypoints\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def answer(ponto_ocupacao):\n",
    "    global width\n",
    "    ocupacao = round((ponto_ocupacao*100)/width)\n",
    "    if ocupacao < 15: return\n",
    "    if ocupacao < 30: return 'A'\n",
    "    if ocupacao < 50: return 'B'\n",
    "    if ocupacao < 70: return 'C'\n",
    "    if ocupacao < 90: return 'D'\n",
    "    return 'E'\n",
    "\n",
    "def coordinates_question(img):\n",
    "\n",
    "    # Criando o detector baseado na versão do CV\n",
    "    is_cv3 = cv2.__version__.startswith(\"3.\")\n",
    "    if is_cv3:\n",
    "        detector = cv2.SimpleBlobDetector_create()\n",
    "    else:\n",
    "        detector = cv2.SimpleBlobDetector()\n",
    "     \n",
    "    #Detectando corpos\n",
    "    keypoints = detector.detect(img)    \n",
    "    # Criando nova imagem com os elementos identificandos pelo detector\n",
    "    im_with_keypoints = cv2.drawKeypoints(img, keypoints, np.array([]),(0,0,255), cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "    cv2.imshow(\"teste\",im_with_keypoints)\n",
    "    cv2.waitKey(0)\n",
    "    return keypoints\n",
    "\n",
    "def evaluate_question(image):\n",
    "    \n",
    "    #global template\n",
    "    \n",
    "    coordinates = coordinates_question(image)  \n",
    "    response = []\n",
    "    if coordinates:\n",
    "        for point in coordinates:            \n",
    "            response.append(answer(point.pt[0]))\n",
    "    im_with_keypoints = cv2.drawKeypoints(template, coordinates, np.array([]),(0,0,255), cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "    return response\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    image = cv2.imread('dataset/imgs/gabarito_template_geral_preenchido.png')\n",
    "    template = cv2.imread('dataset/imgs/gabarito_template_geral_alternativas.png',0)\n",
    "\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.adaptiveThreshold(gray,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY,11,2)\n",
    "    \n",
    "    template = cv2.adaptiveThreshold(template,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY,11,2)\n",
    "    \n",
    "    cv2.imshow(\"template\",template)\n",
    "    cv2.waitKey(0)\n",
    "    \n",
    "    cv2.imshow(\"gray\",gray)\n",
    "    cv2.waitKey(0)\n",
    "    result = cv2.matchTemplate(gray, template, cv2.TM_CCOEFF)\n",
    "    min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)\n",
    "    \n",
    "    height, width = template.shape[:2]    \n",
    "    #Create Bounding Box\n",
    "    top_left = max_loc\n",
    "    bottom_right = (top_left[0] + width, top_left[1] + height)\n",
    "    #cv2.rectangle(image, top_left, bottom_right, (0,0,255), 2)\n",
    "    answer_values = {}\n",
    "    for i in range(0,10):\n",
    "        bottom_right = (bottom_right[0], bottom_right[1] + height)\n",
    "        top_left = (top_left[0],top_left[1]+height)\n",
    "        cv2.rectangle(image, top_left, bottom_right, (0,0,255), 2)\n",
    "        \n",
    "        cv2.imshow(\"image\",image)\n",
    "        cv2.waitKey(0)\n",
    "        \n",
    "        cropped = image[top_left[1]:bottom_right[1],top_left[0]:bottom_right[0]]\n",
    "\n",
    "        answer_values[i+1] = evaluate_question(cropped)\n",
    "\n",
    "    print(answer_values)\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
